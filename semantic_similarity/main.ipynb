{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa43265f",
   "metadata": {},
   "source": [
    "# Semantic Retrieval Model Pipeline\n",
    "\n",
    "Notebook ini membangun model semantic retrieval berbasis FAISS dan SentenceTransformer dari dua dataset MBPP (real & clone).\n",
    "\n",
    "**Alur utama:**\n",
    "1. **Data Loading:** Gabungkan dua dataset JSON ke DataFrame.\n",
    "2. **Embedding:** Buat embedding prompt dengan model multilingual.\n",
    "3. **Indexing:** Normalisasi embedding dan buat FAISS IndexFlatIP untuk similarity search.\n",
    "4. **Model Class:** Bungkus DataFrame, index, dan model ke dalam class `SemanticRetrievalModel`.\n",
    "5. **Simpan Model:** Simpan retrieval model ke file `semantic_retrieval_model.pkl`.\n",
    "6. **Evaluasi:** Pipeline evaluasi (precision, recall, f1, accuracy) dan hyperparameter tuning.\n",
    "7. **Contoh Penggunaan:** Cara menggunakan model hasil PKL.\n",
    "\n",
    "---\n",
    "\n",
    "## Penjelasan Komponen Penting\n",
    "- **FAISS IndexFlatIP:**\n",
    "  - Index similarity berbasis inner product (dot product) yang efisien untuk pencarian embedding.\n",
    "- **Normalisasi Embedding:**\n",
    "  - Membuat vektor embedding menjadi unit norm agar inner product setara dengan cosine similarity.\n",
    "- **SentenceTransformer:**\n",
    "  - Model pre-trained untuk menghasilkan embedding kalimat multibahasa.\n",
    "- **joblib:**\n",
    "  - Untuk serialisasi (save/load) model Python ke file PKL.\n",
    "- **search(query, top_k):**\n",
    "  - Fungsi utama untuk mencari top-k prompt paling mirip secara semantik.\n",
    "- **Pipeline Evaluasi:**\n",
    "  - Mengukur performa retrieval dengan metrik klasifikasi (f1, precision, recall, accuracy).\n",
    "- **Hyperparameter tuning:**\n",
    "  - Mencari nilai top_k terbaik untuk retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe6c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\programdata\\anaconda3\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\yehezkiel david s\\appdata\\roaming\\python\\python313\\site-packages (1.12.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\yehezkiel david s\\appdata\\roaming\\python\\python313\\site-packages (0.2.1)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: langdetect in c:\\users\\yehezkiel david s\\appdata\\roaming\\python\\python313\\site-packages (1.0.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yehezkiel david s\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\yehezkiel david s\\appdata\\roaming\\python\\python313\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yehezkiel david s\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install sentence-transformers faiss-cpu joblib sentencepiece transformers langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6d9743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 583 unique prompts after deduplication.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and merge datasets with deduplication and normalization\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "with open('mbpp_real.json', 'r', encoding='utf-8') as f:\n",
    "    real_data = json.load(f)\n",
    "with open('mbpp_clone.json', 'r', encoding='utf-8') as f:\n",
    "    clone_data = json.load(f)\n",
    "\n",
    "def to_dataframe(data):\n",
    "    if isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        return pd.DataFrame(list(data.values()))\n",
    "    raise ValueError('Unknown data format')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df_real = to_dataframe(real_data)[['prompt', 'code']]\n",
    "df_clone = to_dataframe(clone_data)[['prompt', 'code']]\n",
    "df = pd.concat([df_real, df_clone], ignore_index=True)\n",
    "df['prompt'] = df['prompt'].apply(normalize_text)\n",
    "df = df.drop_duplicates(subset='prompt').reset_index(drop=True)\n",
    "print(f'Loaded {len(df)} unique prompts after deduplication.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9bd80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 583 unique prompts after deduplication.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and merge datasets with deduplication and normalization\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "with open('mbpp_real.json', 'r', encoding='utf-8') as f:\n",
    "    real_data = json.load(f)\n",
    "with open('mbpp_clone.json', 'r', encoding='utf-8') as f:\n",
    "    clone_data = json.load(f)\n",
    "\n",
    "def to_dataframe(data):\n",
    "    if isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        return pd.DataFrame(list(data.values()))\n",
    "    raise ValueError('Unknown data format')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df_real = to_dataframe(real_data)[['prompt', 'code']]\n",
    "df_clone = to_dataframe(clone_data)[['prompt', 'code']]\n",
    "df = pd.concat([df_real, df_clone], ignore_index=True)\n",
    "df['prompt'] = df['prompt'].apply(normalize_text)\n",
    "df = df.drop_duplicates(subset='prompt').reset_index(drop=True)\n",
    "print(f'Loaded {len(df)} unique prompts after deduplication.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7136db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 632 rows, embedding shape: (632, 2304)\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame & embeddings langsung dari JSON (tanpa generate ulang)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('mbpp_all_with_embedding.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "# Pastikan kolom embedding sudah ada dan bentuknya list of list\n",
    "embeddings = np.array(df['embedding'].tolist())\n",
    "print(f\"Loaded {len(df)} rows, embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52feba91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>code</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a function to find the shared elements f...</td>\n",
       "      <td>def similar_elements(test_tup1, test_tup2):\\n ...</td>\n",
       "      <td>[-0.08133774250745773, -0.04932917654514313, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a python function to identify non-prime ...</td>\n",
       "      <td>import math\\ndef is_not_prime(n):\\n    result ...</td>\n",
       "      <td>[0.006655762437731028, 0.13973437249660492, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a function to find the n largest integer...</td>\n",
       "      <td>import heapq as hq\\ndef heap_queue_largest(num...</td>\n",
       "      <td>[-0.036592114716768265, -0.012798348441720009,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a python function to check whether the t...</td>\n",
       "      <td>def is_Power_Of_Two (x): \\n    return x and (n...</td>\n",
       "      <td>[-0.010518835857510567, -0.09875363111495972, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a function to find all words which are a...</td>\n",
       "      <td>import re\\ndef find_char_long(text):\\n  return...</td>\n",
       "      <td>[-0.07087428867816925, -0.053315650671720505, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Write a function to find the shared elements f...   \n",
       "1  Write a python function to identify non-prime ...   \n",
       "2  Write a function to find the n largest integer...   \n",
       "3  Write a python function to check whether the t...   \n",
       "4  Write a function to find all words which are a...   \n",
       "\n",
       "                                                code  \\\n",
       "0  def similar_elements(test_tup1, test_tup2):\\n ...   \n",
       "1  import math\\ndef is_not_prime(n):\\n    result ...   \n",
       "2  import heapq as hq\\ndef heap_queue_largest(num...   \n",
       "3  def is_Power_Of_Two (x): \\n    return x and (n...   \n",
       "4  import re\\ndef find_char_long(text):\\n  return...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.08133774250745773, -0.04932917654514313, -...  \n",
       "1  [0.006655762437731028, 0.13973437249660492, -0...  \n",
       "2  [-0.036592114716768265, -0.012798348441720009,...  \n",
       "3  [-0.010518835857510567, -0.09875363111495972, ...  \n",
       "4  [-0.07087428867816925, -0.053315650671720505, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c8a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2a. Load three best embedding models and translation pipeline (with language detection)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "model1 = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "model2 = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "model3 = SentenceTransformer('intfloat/multilingual-e5-base')\n",
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-id-en', device=0 if torch.cuda.is_available() else -1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a206f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights: (0.5, 0.5, 1.5), mean top-3 similarity: 0.8630\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate prompt embeddings (engineered: per-model normalization, weighted, concat, final normalization)\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def translate_if_needed(text):\n",
    "    \"\"\"Translate Indonesian text to English if needed.\"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except Exception:\n",
    "        lang = 'en'\n",
    "    if lang == 'id':\n",
    "        return translator(text)[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "def get_ensemble_embedding(text, weights):\n",
    "    \"\"\"\n",
    "    Generate ensemble embedding for a single text using per-model normalization and weighting.\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        weights (tuple/list): Weights for each model (length 3).\n",
    "    Returns:\n",
    "        np.ndarray: Normalized ensemble embedding (shape: [1, total_dim]).\n",
    "    \"\"\"\n",
    "    text_en = translate_if_needed(text)\n",
    "    emb1 = model1.encode([text_en], convert_to_numpy=True)\n",
    "    emb2 = model2.encode([text_en], convert_to_numpy=True)\n",
    "    emb3 = model3.encode([text_en], convert_to_numpy=True)\n",
    "    emb1 = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)\n",
    "    emb2 = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)\n",
    "    emb3 = emb3 / np.linalg.norm(emb3, axis=1, keepdims=True)\n",
    "    emb1 = emb1 * weights[0]\n",
    "    emb2 = emb2 * weights[1]\n",
    "    emb3 = emb3 * weights[2]\n",
    "    emb = np.concatenate([emb1, emb2, emb3], axis=1)\n",
    "    emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    return emb\n",
    "\n",
    "def generate_all_embeddings(df, weights):\n",
    "    \"\"\"Generate embeddings for all prompts in the DataFrame using the specified weights.\"\"\"\n",
    "    prompts = df['prompt'].tolist()\n",
    "    return np.vstack([get_ensemble_embedding(p, weights=weights) for p in prompts])\n",
    "\n",
    "def tune_weights(df, model1, model2, model3, translator, weight_grid=None, N=3, sample_size=100):\n",
    "    \"\"\"\n",
    "    Grid search to find the best weights for ensemble embedding.\n",
    "    Returns the best weights tuple.\n",
    "    \"\"\"\n",
    "    if weight_grid is None:\n",
    "        grid = [0.5, 1.0, 1.5]\n",
    "        weight_grid = list(product(grid, repeat=3))\n",
    "    best_score = -1\n",
    "    best_weights = (1.0, 1.0, 1.0)\n",
    "    sample_idx = np.random.choice(len(df), min(sample_size, len(df)), replace=False)\n",
    "    prompts = df.iloc[sample_idx]['prompt'].tolist()\n",
    "    for weights in weight_grid:\n",
    "        emb_sample = np.vstack([get_ensemble_embedding(p, weights=weights) for p in prompts])\n",
    "        emb_sample = emb_sample / np.linalg.norm(emb_sample, axis=1, keepdims=True)\n",
    "        sim = cosine_similarity(emb_sample)\n",
    "        np.fill_diagonal(sim, -1)\n",
    "        topk_sim = np.sort(sim, axis=1)[:, -N:]\n",
    "        avg_sim = np.mean(topk_sim)\n",
    "        if avg_sim > best_score:\n",
    "            best_score = avg_sim\n",
    "            best_weights = weights\n",
    "    print(f'Best weights: {best_weights}, mean top-{N} similarity: {best_score:.4f}')\n",
    "    return best_weights\n",
    "\n",
    "# Pipeline: tune weights first, then generate embeddings\n",
    "best_weights = tune_weights(df, model1, model2, model3, translator)\n",
    "embeddings = generate_all_embeddings(df, weights=best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da11fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m     emb = emb / np.linalg.norm(emb, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m emb\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m best_weights = \u001b[43mtune_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m prompts = df[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m     57\u001b[39m embeddings = np.vstack([get_ensemble_embedding(p, weights=best_weights) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtune_weights\u001b[39m\u001b[34m(df, model1, model2, model3, translator, weight_grid, N, sample_size)\u001b[39m\n\u001b[32m     23\u001b[39m prompts = df.iloc[sample_idx][\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m].tolist()\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m weights \u001b[38;5;129;01min\u001b[39;00m weight_grid:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     emb_sample = np.vstack([\u001b[43mget_ensemble_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts])\n\u001b[32m     26\u001b[39m     emb_sample = emb_sample / np.linalg.norm(emb_sample, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     27\u001b[39m     sim = cosine_similarity(emb_sample)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mget_ensemble_embedding\u001b[39m\u001b[34m(text, weights)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_ensemble_embedding\u001b[39m(text, weights=[\u001b[32m1.0\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m1.0\u001b[39m]):\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Step A: Normalisasi per-model\u001b[39;00m\n\u001b[32m     39\u001b[39m     text_en = translate_if_needed(text)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     emb1 = \u001b[43mmodel1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_en\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     emb2 = model2.encode([text_en], convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     42\u001b[39m     emb3 = model3.encode([text_en], convert_to_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:853\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    846\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    850\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    851\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    867\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:607\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    603\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    605\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:544\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    541\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    542\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:552\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:466\u001b[39m, in \u001b[36mXLMRobertaIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2. Generate prompt embeddings (engineered: per-model normalization, weighted, concat, final normalization)\n",
    "import numpy as np\n",
    "\n",
    "def translate_if_needed(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except Exception:\n",
    "        lang = 'en'\n",
    "    if lang == 'id':\n",
    "        return translator(text)[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# --- Weight tuning grid search ---\n",
    "def tune_weights(df, model1, model2, model3, translator, weight_grid=None, N=3, sample_size=100):\n",
    "    if weight_grid is None:\n",
    "        grid = [0.5, 1.0, 1.5]\n",
    "        weight_grid = list(product(grid, repeat=3))\n",
    "    best_score = -1\n",
    "    best_weights = (1.0, 1.0, 1.0)\n",
    "    sample_idx = np.random.choice(len(df), min(sample_size, len(df)), replace=False)\n",
    "    prompts = df.iloc[sample_idx]['prompt'].tolist()\n",
    "    for weights in weight_grid:\n",
    "        emb_sample = np.vstack([get_ensemble_embedding(p, weights=weights) for p in prompts])\n",
    "        emb_sample = emb_sample / np.linalg.norm(emb_sample, axis=1, keepdims=True)\n",
    "        sim = cosine_similarity(emb_sample)\n",
    "        np.fill_diagonal(sim, -1)\n",
    "        topk_sim = np.sort(sim, axis=1)[:, -N:]\n",
    "        avg_sim = np.mean(topk_sim)\n",
    "        if avg_sim > best_score:\n",
    "            best_score = avg_sim\n",
    "            best_weights = weights\n",
    "    print(f'Best weights: {best_weights}, mean top-{N} similarity: {best_score:.4f}')\n",
    "    return best_weights\n",
    "\n",
    "def get_ensemble_embedding(text, weights=[1.0, 1.0, 1.0]):\n",
    "    # Step A: Normalisasi per-model\n",
    "    text_en = translate_if_needed(text)\n",
    "    emb1 = model1.encode([text_en], convert_to_numpy=True)\n",
    "    emb2 = model2.encode([text_en], convert_to_numpy=True)\n",
    "    emb3 = model3.encode([text_en], convert_to_numpy=True)\n",
    "    emb1 = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)\n",
    "    emb2 = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)\n",
    "    emb3 = emb3 / np.linalg.norm(emb3, axis=1, keepdims=True)\n",
    "    # Step B: Apply weight before concatenation\n",
    "    emb1 = emb1 * weights[0]\n",
    "    emb2 = emb2 * weights[1]\n",
    "    emb3 = emb3 * weights[2]\n",
    "    emb = np.concatenate([emb1, emb2, emb3], axis=1)\n",
    "    # Step C: Normalisasi ulang embedding final\n",
    "    emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    return emb\n",
    "\n",
    "best_weights = tune_weights(df, model1, model2, model3, translator)\n",
    "prompts = df['prompt'].tolist()\n",
    "embeddings = np.vstack([get_ensemble_embedding(p, weights=best_weights) for p in prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a99436a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan embedding ke file JSON baru (gabungan real+clone+embedding)\n",
    "import json\n",
    "df['embedding'] = [emb.tolist() for emb in embeddings]  # pastikan urutan sama\n",
    "with open('mbpp_all_with_embedding_v2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8de0b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>code</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a function to find the shared elements f...</td>\n",
       "      <td>def similar_elements(test_tup1, test_tup2):\\n ...</td>\n",
       "      <td>[-0.00838712602853775, -0.005086569115519524, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a python function to identify non-prime ...</td>\n",
       "      <td>import math\\ndef is_not_prime(n):\\n    result ...</td>\n",
       "      <td>[0.0007013169233687222, 0.014723795466125011, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a function to find the n largest integer...</td>\n",
       "      <td>import heapq as hq\\ndef heap_queue_largest(num...</td>\n",
       "      <td>[-0.004016818478703499, -0.0014049103483557701...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a python function to check whether the t...</td>\n",
       "      <td>def is_Power_Of_Two (x): \\n    return x and (n...</td>\n",
       "      <td>[-0.0011347393738105893, -0.010653235018253326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a function to find all words which are a...</td>\n",
       "      <td>import re\\ndef find_char_long(text):\\n  return...</td>\n",
       "      <td>[-0.007360551971942186, -0.005537023767828941,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Write a function to find the shared elements f...   \n",
       "1  Write a python function to identify non-prime ...   \n",
       "2  Write a function to find the n largest integer...   \n",
       "3  Write a python function to check whether the t...   \n",
       "4  Write a function to find all words which are a...   \n",
       "\n",
       "                                                code  \\\n",
       "0  def similar_elements(test_tup1, test_tup2):\\n ...   \n",
       "1  import math\\ndef is_not_prime(n):\\n    result ...   \n",
       "2  import heapq as hq\\ndef heap_queue_largest(num...   \n",
       "3  def is_Power_Of_Two (x): \\n    return x and (n...   \n",
       "4  import re\\ndef find_char_long(text):\\n  return...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.00838712602853775, -0.005086569115519524, ...  \n",
       "1  [0.0007013169233687222, 0.014723795466125011, ...  \n",
       "2  [-0.004016818478703499, -0.0014049103483557701...  \n",
       "3  [-0.0011347393738105893, -0.010653235018253326...  \n",
       "4  [-0.007360551971942186, -0.005537023767828941,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f3d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Normalize embeddings and build FAISS index (ensemble)\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c1e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define SemanticRetrievalModel class (portable, with dummy embedding option)\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SemanticRetrievalModel:\n",
    "    def __init__(self, df, index, embeddings, encoder_func, best_k=5, weights=None):\n",
    "        self.df = df\n",
    "        self.index = index\n",
    "        self.embeddings = embeddings\n",
    "        self.encoder_func = encoder_func\n",
    "        self.best_k = best_k\n",
    "        self.weights = weights\n",
    "\n",
    "    def search(self, query: str, top_k: int = None, weights=None):\n",
    "        if top_k is None:\n",
    "            top_k = self.best_k\n",
    "        if weights is None:\n",
    "            weights = self.weights\n",
    "        emb = self.encoder_func(query, weights=weights)\n",
    "        emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        D, I = self.index.search(emb, top_k)\n",
    "        results = self.df.iloc[I[0]].copy()\n",
    "        results['score'] = D[0]\n",
    "        return results[['prompt', 'score', 'code']]\n",
    "\n",
    "\n",
    "\n",
    "# --- Embedding function: engineered (per-model normalization, weighted, concat, final normalization) ---\n",
    "def get_ensemble_embedding(text, weights):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except Exception:\n",
    "        lang = 'en'\n",
    "    if lang == 'id':\n",
    "        text = translator(text)[0]['translation_text']\n",
    "    emb1 = model1.encode([text], convert_to_numpy=True)\n",
    "    emb2 = model2.encode([text], convert_to_numpy=True)\n",
    "    emb3 = model3.encode([text], convert_to_numpy=True)\n",
    "    emb1 = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)\n",
    "    emb2 = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)\n",
    "    emb3 = emb3 / np.linalg.norm(emb3, axis=1, keepdims=True)\n",
    "    emb1 = emb1 * weights[0]\n",
    "    emb2 = emb2 * weights[1]\n",
    "    emb3 = emb3 * weights[2]\n",
    "    emb = np.concatenate([emb1, emb2, emb3], axis=1)\n",
    "    emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    return emb\n",
    "\n",
    "# --- Embedding function: dummy (for PKL loading/testing anywhere) ---\n",
    "def dummy_embedding(text):\n",
    "    # Return a fixed-size zero vector matching the real embedding shape\n",
    "    # (replace 1536 with your real embedding dim)\n",
    "    return np.zeros((1, 1536), dtype=np.float32)\n",
    "\n",
    "\n",
    "retrieval_model = SemanticRetrievalModel(df, index, embeddings, get_ensemble_embedding, weights=best_weights)\n",
    "# retrieval_model = SemanticRetrievalModel(df, index, embeddings, dummy_embedding)  # Uncomment for dummy mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb5a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 632/632 [02:08<00:00,  4.90it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh mapping relevansi (prompt, relevant prompts, avg similarity):\n",
      "Query: Write a function to find minimum of three numbers.\n",
      "Relevant: ['Write a function to find the median of three numbers.', 'Develop a function that returns the smallest value among three given numbers.', 'Write a python function to find the minimum of two numbers.']\n",
      "Avg sim: 0.915\n",
      "---\n",
      "Query: Develop a function that checks if every dictionary in a list is empty.\n",
      "Relevant: ['Write a function to check whether all dictionaries in a list are empty or not.', 'Create a function that verifies if a dictionary is empty.', 'Create a function that verifies if a dictionary is empty.']\n",
      "Avg sim: 0.949\n",
      "---\n",
      "Query: Write a function that takes in an array and an integer n, and re-arranges the first n elements of the given array so that all negative elements appear before positive ones, and where the relative order among negative and positive elements is preserved.\n",
      "Relevant: ['Create a function that moves all negative numbers to the front of the first n elements of an array, keeping their order.', 'Write a function that takes in a list and an integer n and splits a list for every nth element, returning a list of the resulting lists.', 'Write a function that takes in a sorted array, its length (n), and an element and returns whether the element is the majority element in the given sorted array. (The majority element is the element that occurs more than n/2 times.)']\n",
      "Avg sim: 0.878\n",
      "---\n",
      "Query: Create a function that counts how many elements appear before the first tuple element in the given tuple.\n",
      "Relevant: ['Write a function to find the number of elements that occurs before the tuple element in the given tuple.', 'Create a function that tallies the occurrences of each list element in a tuple.', 'Create a function that identifies the shared first element in a list of tuples.']\n",
      "Avg sim: 0.917\n",
      "---\n",
      "Query: Create a function that determines if a number is even.\n",
      "Relevant: ['Create a function that determines if the parity of a number is odd.', 'Write a python function to check whether the given number is even or not.', 'Create a function that determines whether a given integer is prime.']\n",
      "Avg sim: 0.908\n",
      "---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 3  # Atur N sesuai kebutuhan\n",
    "relevance_dict = {}\n",
    "similarity_scores = []  # Store average similarity for each query\n",
    "topN_prompts = []  # Store top-N prompt texts for each query\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row['prompt']\n",
    "    results = retrieval_model.search(query, top_k=N+1)  # +1 untuk menghindari self-match\n",
    "    result_indices = results.index.tolist()\n",
    "    result_scores = results['score'].tolist()\n",
    "    if idx in result_indices:\n",
    "        remove_idx = result_indices.index(idx)\n",
    "        result_indices.pop(remove_idx)\n",
    "        result_scores.pop(remove_idx)\n",
    "    relevance_dict[idx] = result_indices[:N]\n",
    "    similarity_scores.append(np.mean(result_scores[:N]))\n",
    "    topN_prompts.append(df.loc[result_indices[:N], 'prompt'].tolist())\n",
    "\n",
    "# Tambahkan ke DataFrame untuk analisis manual\n",
    "df['relevant_indices'] = df.index.map(relevance_dict)\n",
    "df['avg_topN_similarity'] = similarity_scores\n",
    "df['topN_prompts'] = topN_prompts\n",
    "\n",
    "# Tampilkan contoh mapping relevansi dengan prompt dan skor\n",
    "print('Contoh mapping relevansi (prompt, relevant prompts, avg similarity):')\n",
    "for i in random.sample(range(len(df)), min(5, len(df))):\n",
    "    prompt = df.loc[i, 'prompt']\n",
    "    relevant_prompts = df.loc[i, 'topN_prompts']\n",
    "    avg_sim = df.loc[i, 'avg_topN_similarity']\n",
    "    print(f'Query: {prompt}\\nRelevant: {relevant_prompts}\\nAvg sim: {avg_sim:.3f}\\n---')\n",
    "\n",
    "# Visualisasi distribusi skor similarity\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df['avg_topN_similarity'], bins=20, alpha=0.7, color='royalblue')\n",
    "plt.xlabel('Average Top-N Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Average Top-N Similarity per Query')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da716c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame with relevance mapping to mbpp_all_with_embedding_and_relevance.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save DataFrame with relevance mapping to JSON (for fast reload, skip recompute) ---\n",
    "import json\n",
    "\n",
    "df_to_save = df.copy()\n",
    "# Convert numpy types and lists to native Python types for JSON serialization\n",
    "for col in ['relevant_indices', 'topN_prompts']:\n",
    "    df_to_save[col] = df_to_save[col].apply(lambda x: list(map(int, x)) if col == 'relevant_indices' else x)\n",
    "\n",
    "df_to_save['avg_topN_similarity'] = df_to_save['avg_topN_similarity'].astype(float)\n",
    "\n",
    "with open('mbpp_all_with_embedding_and_relevance_v2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(df_to_save.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Saved DataFrame with relevance mapping to mbpp_all_with_embedding_and_relevance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa29515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/632 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 632/632 [02:06<00:00,  4.99it/s]\n",
      "  0%|          | 0/632 [00:00<?, ?it/s]\n",
      "100%|| 632/632 [02:17<00:00,  4.58it/s]\n",
      "100%|| 632/632 [02:17<00:00,  4.58it/s]\n",
      "100%|| 632/632 [02:07<00:00,  4.94it/s]\n",
      "100%|| 632/632 [02:07<00:00,  4.94it/s]\n",
      "100%|| 632/632 [02:22<00:00,  4.43it/s]\n",
      "100%|| 632/632 [02:22<00:00,  4.43it/s]\n",
      "100%|| 632/632 [02:31<00:00,  4.17it/s]\n",
      "100%|| 632/632 [02:31<00:00,  4.17it/s]\n",
      "100%|| 632/632 [02:33<00:00,  4.11it/s]\n",
      "100%|| 632/632 [02:33<00:00,  4.11it/s]\n",
      "100%|| 632/632 [02:32<00:00,  4.14it/s]\n",
      "100%|| 632/632 [02:32<00:00,  4.14it/s]\n",
      "100%|| 632/632 [02:55<00:00,  3.60it/s]\n",
      "100%|| 632/632 [02:55<00:00,  3.60it/s]\n",
      "100%|| 632/632 [02:38<00:00,  3.98it/s]\n",
      "100%|| 632/632 [02:38<00:00,  3.98it/s]\n",
      "100%|| 632/632 [02:23<00:00,  4.41it/s]\n",
      "100%|| 632/632 [02:23<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best top_k: 2 with F1-score: 1.0000\n",
      "F1-scores by top_k: {1: 0.07317073170731707, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 1.0}\n",
      "Average retrieval diversity (unique prompts per query): 5.50\n",
      "Example error case:\n",
      "Query: Write a function to find the shared elements from the given two lists.\n",
      "Relevant: ['Create a function that returns the elements found in both of the provided lists.', 'Write a function to find the common elements in given nested lists.', 'Develop a function that returns the elements shared by all nested lists.']\n",
      "Retrieved: ['Write a function to find the shared elements from the given two lists.']\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced Evaluation: Retrieval Metrics, Error Analysis, and Diversity ---\n",
    "if 'relevance_dict' not in globals():\n",
    "    raise RuntimeError('Jalankan cell Automatic Relevance Mapping (cell sebelumnya) terlebih dahulu!')\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "\n",
    "best_f1 = 0\n",
    "best_k = 1\n",
    "results_dict = {}\n",
    "error_cases = []\n",
    "diversity_scores = []\n",
    "for top_k in range(1, 11):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        query = row['prompt']\n",
    "        relevant_indices = relevance_dict[idx]\n",
    "        results = retrieval_model.search(query, top_k=top_k)\n",
    "        result_indices = results.index.tolist()\n",
    "        match = any(i in result_indices for i in relevant_indices)\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if match else 0)\n",
    "        # Error analysis: log false negatives\n",
    "        if not match:\n",
    "            error_cases.append({'query': query, 'relevant': [df.loc[i, 'prompt'] for i in relevant_indices], 'retrieved': [df.loc[i, 'prompt'] for i in result_indices]})\n",
    "        # Diversity: unique prompt count in retrieval\n",
    "        diversity_scores.append(len(set(result_indices)))\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    results_dict[top_k] = f1\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_k = top_k\n",
    "print(f'Best top_k: {best_k} with F1-score: {best_f1:.4f}')\n",
    "print('F1-scores by top_k:', results_dict)\n",
    "print(f'Average retrieval diversity (unique prompts per query): {np.mean(diversity_scores):.2f}')\n",
    "if error_cases:\n",
    "    print(f'Example error case:')\n",
    "    print('Query:', error_cases[0]['query'])\n",
    "    print('Relevant:', error_cases[0]['relevant'])\n",
    "    print('Retrieved:', error_cases[0]['retrieved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c92fef81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic_retrieval_mode_rev.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Save the model (with best_k from tuning)\n",
    "# Pastikan variabel best_k dan tqdm sudah didefinisikan sebelum cell ini dijalankan\n",
    "try:\n",
    "    _ = best_k\n",
    "except NameError:\n",
    "    raise RuntimeError('Jalankan cell evaluasi hyperparameter (yang mendefinisikan best_k) terlebih dahulu!')\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    raise ImportError('tqdm belum terinstall. Jalankan !pip install tqdm atau %pip install tqdm')\n",
    "\n",
    "retrieval_model.best_k = best_k\n",
    "import joblib\n",
    "joblib.dump(retrieval_model, 'semantic_retrieval_mode_rev.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "577fd02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                prompt     score  \\\n",
      "139  Write a function to find the maximum value in ...  0.881030   \n",
      "155  Write a function to find the list with maximum...  0.874503   \n",
      "135  Write a function to find the list of maximum l...  0.871245   \n",
      "\n",
      "                                                  code  \n",
      "139  def max_val(listval):\\n     max_val = max(i fo...  \n",
      "155  def max_length_list(input_list):\\n    max_leng...  \n",
      "135  def max_length(list1):\\n    max_length = max(l...  \n"
     ]
    }
   ],
   "source": [
    "# 6. Load and test the saved model with a user query\n",
    "import joblib\n",
    "\n",
    "# Path PKL harus sesuai lokasi file hasil dump\n",
    "loaded_model = joblib.load('semantic_retrieval_mode_rev.pkl')\n",
    "user_query = \"find the maximum value in a list\"\n",
    "results = loaded_model.search(user_query, top_k=3)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6947f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                prompt     score  \\\n",
      "139  Write a function to find the maximum value in ...  0.885769   \n",
      "135  Write a function to find the list of maximum l...  0.875365   \n",
      "155  Write a function to find the list with maximum...  0.874187   \n",
      "\n",
      "                                                  code  \n",
      "139  def max_val(listval):\\n     max_val = max(i fo...  \n",
      "135  def max_length(list1):\\n    max_length = max(l...  \n",
      "155  def max_length_list(input_list):\\n    max_leng...  \n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load('semantic_retrieval_mode_rev.pkl')\n",
    "user_query = \"Temukan nilai maksimum dalam sebuah list\"\n",
    "results = loaded_model.search(user_query, top_k=3)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a06c5f5",
   "metadata": {},
   "source": [
    "##  Cara Pakai & Testing PKL di Lingkungan Manapun\n",
    "\n",
    "- **Production:**\n",
    "  - Pastikan cell model1/model2/model3/translator sudah di-load.\n",
    "  - Gunakan `get_ensemble_embedding` untuk retrieval_model.\n",
    "- **Testing/Portabilitas:**\n",
    "  - Ganti ke `dummy_embedding` agar PKL bisa di-load tanpa dependensi model.\n",
    "  - Cocok untuk test API, CLI, atau deployment tanpa GPU/model besar.\n",
    "\n",
    "**Contoh cell untuk load PKL dan test retrieval:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e41f4703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: carikan saya nil;ai maksimum diantara 3 list\n",
      "SemSim Search:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>score</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Write a function to find the list with maximum length.</td>\n",
       "      <td>0.800187</td>\n",
       "      <td>def max_length_list(input_list):\\n    max_length = max(len(x) for x in input_list )   \\n    max_list = max(input_list, key = lambda i: len(i))    \\n    return(max_length, max_list)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Write a function to find the list of maximum length in a list of lists.</td>\n",
       "      <td>0.793205</td>\n",
       "      <td>def max_length(list1):\\n    max_length = max(len(x) for x in  list1 )  \\n    max_list = max((x) for x in   list1)\\n    return(max_length, max_list)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Write a python function to find element of a list having maximum length.</td>\n",
       "      <td>0.792123</td>\n",
       "      <td>def Find_Max(lst): \\n    maxList = max((x) for x in lst) \\n    return maxList</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       prompt  \\\n",
       "155                    Write a function to find the list with maximum length.   \n",
       "135   Write a function to find the list of maximum length in a list of lists.   \n",
       "182  Write a python function to find element of a list having maximum length.   \n",
       "\n",
       "        score  \\\n",
       "155  0.800187   \n",
       "135  0.793205   \n",
       "182  0.792123   \n",
       "\n",
       "                                                                                                                                                                                     code  \n",
       "155  def max_length_list(input_list):\\n    max_length = max(len(x) for x in input_list )   \\n    max_list = max(input_list, key = lambda i: len(i))    \\n    return(max_length, max_list)  \n",
       "135                                   def max_length(list1):\\n    max_length = max(len(x) for x in  list1 )  \\n    max_list = max((x) for x in   list1)\\n    return(max_length, max_list)  \n",
       "182                                                                                                         def Find_Max(lst): \\n    maxList = max((x) for x in lst) \\n    return maxList  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Load PKL & test retrieval (REAL mode, user input prompt, full table output) ---\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Pastikan cell model1, model2, model3, translator, dan get_ensemble_embedding sudah di-load!\n",
    "loaded_model = joblib.load('semantic_retrieval_mode_rev.pkl')\n",
    "loaded_model.encoder_func = get_ensemble_embedding  # Pakai fungsi embedding real\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 0)\n",
    "\n",
    "user_query = input(\"Masukkan prompt: \")\n",
    "print(\"User:\",user_query)\n",
    "print(\"SemSim Search:\")\n",
    "results = loaded_model.search(user_query, top_k=3)\n",
    "display(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
