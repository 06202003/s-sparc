{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa43265f",
   "metadata": {},
   "source": [
    "# Semantic Retrieval Model Pipeline\n",
    "\n",
    "Notebook ini membangun model semantic retrieval berbasis FAISS dan SentenceTransformer dari dua dataset MBPP (real & clone).\n",
    "\n",
    "**Alur utama:**\n",
    "1. **Data Loading:** Gabungkan dua dataset JSON ke DataFrame.\n",
    "2. **Embedding:** Buat embedding prompt dengan model multilingual.\n",
    "3. **Indexing:** Normalisasi embedding dan buat FAISS IndexFlatIP untuk similarity search.\n",
    "4. **Model Class:** Bungkus DataFrame, index, dan model ke dalam class `SemanticRetrievalModel`.\n",
    "5. **Simpan Model:** Simpan retrieval model ke file `semantic_retrieval_model.pkl`.\n",
    "6. **Evaluasi:** Pipeline evaluasi (precision, recall, f1, accuracy) dan hyperparameter tuning.\n",
    "7. **Contoh Penggunaan:** Cara menggunakan model hasil PKL.\n",
    "\n",
    "---\n",
    "\n",
    "## Penjelasan Komponen Penting\n",
    "- **FAISS IndexFlatIP:**\n",
    "  - Index similarity berbasis inner product (dot product) yang efisien untuk pencarian embedding.\n",
    "- **Normalisasi Embedding:**\n",
    "  - Membuat vektor embedding menjadi unit norm agar inner product setara dengan cosine similarity.\n",
    "- **SentenceTransformer:**\n",
    "  - Model pre-trained untuk menghasilkan embedding kalimat multibahasa.\n",
    "- **joblib:**\n",
    "  - Untuk serialisasi (save/load) model Python ke file PKL.\n",
    "- **search(query, top_k):**\n",
    "  - Fungsi utama untuk mencari top-k prompt paling mirip secara semantik.\n",
    "- **Pipeline Evaluasi:**\n",
    "  - Mengukur performa retrieval dengan metrik klasifikasi (f1, precision, recall, accuracy).\n",
    "- **Hyperparameter tuning:**\n",
    "  - Mencari nilai top_k terbaik untuk retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install sentence-transformers faiss-cpu joblib sentencepiece transformers langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and merge datasets with deduplication and normalization\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "with open('mbpp_real.json', 'r', encoding='utf-8') as f:\n",
    "    real_data = json.load(f)\n",
    "with open('mbpp_clone.json', 'r', encoding='utf-8') as f:\n",
    "    clone_data = json.load(f)\n",
    "\n",
    "def to_dataframe(data):\n",
    "    if isinstance(data, list):\n",
    "        return pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        return pd.DataFrame(list(data.values()))\n",
    "    raise ValueError('Unknown data format')\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "df_real = to_dataframe(real_data)[['prompt', 'code']]\n",
    "df_clone = to_dataframe(clone_data)[['prompt', 'code']]\n",
    "df = pd.concat([df_real, df_clone], ignore_index=True)\n",
    "df['prompt'] = df['prompt'].apply(normalize_text)\n",
    "df = df.drop_duplicates(subset='prompt').reset_index(drop=True)\n",
    "print(f'Loaded {len(df)} unique prompts after deduplication.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame & embeddings langsung dari JSON (tanpa generate ulang)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "with open('mbpp_all_with_embedding.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "# Pastikan kolom embedding sudah ada dan bentuknya list of list\n",
    "embeddings = np.array(df['embedding'].tolist())\n",
    "print(f\"Loaded {len(df)} rows, embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52feba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Load three best embedding models and translation pipeline (with language detection)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "model1 = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "model2 = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "model3 = SentenceTransformer('intfloat/multilingual-e5-base')\n",
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-id-en', device=0 if torch.cuda.is_available() else -1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da11fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate prompt embeddings (ensemble 3 model + auto-translate Indo->En)\n",
    "import numpy as np\n",
    "\n",
    "def translate_if_needed(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = 'en'\n",
    "    if lang == 'id':\n",
    "        return translator(text)[0]['translation_text']\n",
    "    return text\n",
    "\n",
    "def get_ensemble_embedding(text):\n",
    "    text_en = translate_if_needed(text)\n",
    "    emb1 = model1.encode([text_en], convert_to_numpy=True)\n",
    "    emb2 = model2.encode([text_en], convert_to_numpy=True)\n",
    "    emb3 = model3.encode([text_en], convert_to_numpy=True)\n",
    "    # Weighted average ensemble (can be tuned)\n",
    "    emb = np.concatenate([emb1, emb2, emb3], axis=1)\n",
    "    return emb\n",
    "\n",
    "prompts = df['prompt'].tolist()\n",
    "embeddings = np.vstack([get_ensemble_embedding(p) for p in prompts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99436a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan embedding ke file JSON baru (gabungan real+clone+embedding)\n",
    "import json\n",
    "df['embedding'] = [emb.tolist() for emb in embeddings]  # pastikan urutan sama\n",
    "with open('mbpp_all_with_embedding.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Normalize embeddings and build FAISS index (ensemble)\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define SemanticRetrievalModel class (ensemble 3 model + auto-translate)\n",
    "import joblib\n",
    "\n",
    "class SemanticRetrievalModel:\n",
    "    def __init__(self, df, index, embeddings, encoder_func, best_k=5):\n",
    "        self.df = df\n",
    "        self.index = index\n",
    "        self.embeddings = embeddings\n",
    "        self.encoder_func = encoder_func\n",
    "        self.best_k = best_k\n",
    "\n",
    "    def search(self, query: str, top_k: int = None):\n",
    "        if top_k is None:\n",
    "            top_k = self.best_k\n",
    "        emb = self.encoder_func(query)\n",
    "        emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        D, I = self.index.search(emb, top_k)\n",
    "        results = self.df.iloc[I[0]].copy()\n",
    "        results['score'] = D[0]\n",
    "        return results[['prompt', 'score', 'code']]\n",
    "\n",
    "retrieval_model = SemanticRetrievalModel(df, index, embeddings, get_ensemble_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 3  # Atur N sesuai kebutuhan\n",
    "relevance_dict = {}\n",
    "similarity_scores = []  # Store average similarity for each query\n",
    "topN_prompts = []  # Store top-N prompt texts for each query\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    query = row['prompt']\n",
    "    results = retrieval_model.search(query, top_k=N+1)  # +1 untuk menghindari self-match\n",
    "    result_indices = results.index.tolist()\n",
    "    result_scores = results['score'].tolist()\n",
    "    if idx in result_indices:\n",
    "        remove_idx = result_indices.index(idx)\n",
    "        result_indices.pop(remove_idx)\n",
    "        result_scores.pop(remove_idx)\n",
    "    relevance_dict[idx] = result_indices[:N]\n",
    "    similarity_scores.append(np.mean(result_scores[:N]))\n",
    "    topN_prompts.append(df.loc[result_indices[:N], 'prompt'].tolist())\n",
    "\n",
    "# Tambahkan ke DataFrame untuk analisis manual\n",
    "df['relevant_indices'] = df.index.map(relevance_dict)\n",
    "df['avg_topN_similarity'] = similarity_scores\n",
    "df['topN_prompts'] = topN_prompts\n",
    "\n",
    "# Tampilkan contoh mapping relevansi dengan prompt dan skor\n",
    "print('Contoh mapping relevansi (prompt, relevant prompts, avg similarity):')\n",
    "for i in random.sample(range(len(df)), min(5, len(df))):\n",
    "    prompt = df.loc[i, 'prompt']\n",
    "    relevant_prompts = df.loc[i, 'topN_prompts']\n",
    "    avg_sim = df.loc[i, 'avg_topN_similarity']\n",
    "    print(f'Query: {prompt}\\nRelevant: {relevant_prompts}\\nAvg sim: {avg_sim:.3f}\\n---')\n",
    "\n",
    "# Visualisasi distribusi skor similarity\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(df['avg_topN_similarity'], bins=20, alpha=0.7, color='royalblue')\n",
    "plt.xlabel('Average Top-N Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Average Top-N Similarity per Query')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da716c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save DataFrame with relevance mapping to JSON (for fast reload, skip recompute) ---\n",
    "import json\n",
    "\n",
    "df_to_save = df.copy()\n",
    "# Convert numpy types and lists to native Python types for JSON serialization\n",
    "for col in ['relevant_indices', 'topN_prompts']:\n",
    "    df_to_save[col] = df_to_save[col].apply(lambda x: list(map(int, x)) if col == 'relevant_indices' else x)\n",
    "\n",
    "df_to_save['avg_topN_similarity'] = df_to_save['avg_topN_similarity'].astype(float)\n",
    "\n",
    "with open('mbpp_all_with_embedding_and_relevance.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(df_to_save.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Saved DataFrame with relevance mapping to mbpp_all_with_embedding_and_relevance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced Evaluation: Retrieval Metrics, Error Analysis, and Diversity ---\n",
    "if 'relevance_dict' not in globals():\n",
    "    raise RuntimeError('Jalankan cell Automatic Relevance Mapping (cell sebelumnya) terlebih dahulu!')\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "\n",
    "best_f1 = 0\n",
    "best_k = 1\n",
    "results_dict = {}\n",
    "error_cases = []\n",
    "diversity_scores = []\n",
    "for top_k in range(1, 11):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        query = row['prompt']\n",
    "        relevant_indices = relevance_dict[idx]\n",
    "        results = retrieval_model.search(query, top_k=top_k)\n",
    "        result_indices = results.index.tolist()\n",
    "        match = any(i in result_indices for i in relevant_indices)\n",
    "        y_true.append(1)\n",
    "        y_pred.append(1 if match else 0)\n",
    "        # Error analysis: log false negatives\n",
    "        if not match:\n",
    "            error_cases.append({'query': query, 'relevant': [df.loc[i, 'prompt'] for i in relevant_indices], 'retrieved': [df.loc[i, 'prompt'] for i in result_indices]})\n",
    "        # Diversity: unique prompt count in retrieval\n",
    "        diversity_scores.append(len(set(result_indices)))\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    results_dict[top_k] = f1\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_k = top_k\n",
    "print(f'Best top_k: {best_k} with F1-score: {best_f1:.4f}')\n",
    "print('F1-scores by top_k:', results_dict)\n",
    "print(f'Average retrieval diversity (unique prompts per query): {np.mean(diversity_scores):.2f}')\n",
    "if error_cases:\n",
    "    print(f'Example error case:')\n",
    "    print('Query:', error_cases[0]['query'])\n",
    "    print('Relevant:', error_cases[0]['relevant'])\n",
    "    print('Retrieved:', error_cases[0]['retrieved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save the model (with best_k from tuning)\n",
    "# Pastikan variabel best_k dan tqdm sudah didefinisikan sebelum cell ini dijalankan\n",
    "try:\n",
    "    _ = best_k\n",
    "except NameError:\n",
    "    raise RuntimeError('Jalankan cell evaluasi hyperparameter (yang mendefinisikan best_k) terlebih dahulu!')\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    raise ImportError('tqdm belum terinstall. Jalankan !pip install tqdm atau %pip install tqdm')\n",
    "\n",
    "retrieval_model.best_k = best_k\n",
    "import joblib\n",
    "joblib.dump(retrieval_model, 'semantic_retrieval_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fd02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load and test the saved model with a user query\n",
    "import joblib\n",
    "\n",
    "# Path PKL harus sesuai lokasi file hasil dump\n",
    "loaded_model = joblib.load('semantic_retrieval_model.pkl')\n",
    "user_query = \"find the maximum value in a list\"\n",
    "results = loaded_model.search(user_query, top_k=3)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('semantic_retrieval_model.pkl')\n",
    "user_query = \"Temukan nilai maksimum dalam sebuah list\"\n",
    "results = loaded_model.search(user_query, top_k=3)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
